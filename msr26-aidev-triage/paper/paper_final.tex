\documentclass[sigconf,review,anonymous]{acmart}
\acmConference[MSR 2026]{MSR '26: Proceedings of the 23rd International Conference on Mining Software Repositories}{April 2026}{Rio de Janeiro, Brazil}

\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference info
\setcopyright{none}
\copyrightyear{2026}
\acmYear{2026}
\acmDOI{None}

\begin{document}

\title{Ghosting in the Machine: Predicting Wasted Review Effort in AI-Generated Pull Requests}

\author{Anonymous Author(s)}
\affiliation{
  \institution{MSR 2026 Mining Challenge}
  \country{Brazil}
}
\email{anonymous@example.com}

\renewcommand{\shortauthors}{Anonymous et al.}

\begin{abstract}
The emergence of autonomous coding agents has introduced a new dynamic in software engineering: "AI Teammates" that independently author Pull Requests (PRs). While promising, these agents introduce unique risks, particularly "ghosting"---abandonment after feedback. In this study, we analyze 33,596 Agentic-PRs from the AIDev dataset to characterize this phenomenon. We identify two distinct regimes: "Instant Merges" (32\%) which are narrow-scope updates (median 68 lines), and "Normal PRs" where agents face genuine complexity. Our LightGBM models achieve an AUC of 0.84 for identifying high-cost PRs, outperforming a text baseline (AUC 0.57) and generalizing across unseen agents (LOAO AUC 0.66--0.80). Furthermore, we demonstrate that triage policies prioritizing the top 20\% of risky PRs can capture 47.4\% of total review effort on a repo-disjoint test set. These findings emphasize the importance of structural signals in automated triage and propose actionable human-in-the-loop workflows to mitigate the hidden costs of AI collaboration.
\end{abstract}

\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10011007.10011074.10011111.10011113</concept_id>
  <concept_desc>Software and its engineering~Software evolution</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Software evolution}

\keywords{AI Agents, Triage, Ghosting, Mining Software Repositories}

\maketitle

\section{Introduction}
In the rapidly evolving landscape of Modern Software Engineering, the role of Artificial Intelligence (AI) has shifted from passive assistance to active participation. The emergence of autonomous coding agents---``AI Teammates'' capable of independently planning, coding, and submitting Pull Requests (PRs)---marks a paradigm shift in collaborative development \cite{li2025aidev,peng2023copilot,chen2021codex,brown2020language,touvron2023llama,li2022competition,fried2023incoder,austin2021program}. Tools like GitHub Copilot Workspace, Devin, and OpenHands promise to accelerate development cycles and reduce the burden of mundane tasks \cite{ziegler2022productivity,barke2023grounded,vaithilingam2022usability}. However, this autonomy introduces new friction points in the human-AI workflow. Unlike human contributors, who typically adhere to social norms of communication and stewardship \cite{rigby2013convergent,thongtanunam2017review,dabbish2012social,steinmacher2015barriers}, early autonomous agents often exhibit erratic follow-through behavior, a phenomenon we term ``Ghosting.''

Ghosting occurs when an agent submits a PR but fails to respond to human feedback or CI failures, effectively abandoning the contribution. This behavior imposes a significant ``Hidden Cost'' on open-source maintainers, who must invest time reviewing code, understanding intent, and providing feedback, only to have that effort wasted \cite{bacchelli2013expectations,mcintosh2014impact}. As Agentic-PRs become ubiquitous, the risk of a ``Denial-of-Service'' attack on maintainer attention becomes acute. Existing research on Pull Request triage has largely focused on human-centric metrics (e.g., social reputation, prior contributions) \cite{tsay2014pr,gousios2014pr,yu2015wait,yu2016reviewer,choetkiertikul2015predicting,gousios2015work,gousios2016maintainer}. However, AI agents lack social accountability and operate under different constraints---often prioritizing speed and volume over correctness or maintainability \cite{vasilescu2015quality,kim2008classifying,rahman2012recalling}. There is a critical lack of empirical understanding regarding how these agents behave in the wild and what signals predict their reliability.

To address this gap, we present a comprehensive study of 33,596 PRs authored by five prominent AI agents (Claude, Copilot, Cursor, Devin, Codex) from the AIDev dataset \cite{li2025aidev}. We aim to operationalize the concept of ``Agentic Ghosting'' and develop predictive mechanisms to triage high-risk contributions before they consume scarce reviewer resources \cite{bird2009promises,kalliamvakou2014promises,treude2011programmers,store2019bot,wessel2020bots,wessel2021effects}. Specifically, we investigate:
\begin{itemize}
    \item \textbf{RQ1 (Predictability)}: To what extent can we rely on submission-time signals to predict which Agentic-PRs will incur high review costs or be abandoned?
    \item \textbf{RQ2 (Risk Factors)}: What behavioral and structural cues---such as file complexity or interaction patterns---signal a higher propensity for ghosting?
\end{itemize}

Our contributions are threefold:
\begin{enumerate}
    \item \textbf{Operationalization of Ghosting}: We establish a rigorous definition of ``True Ghosting'' (abandonment after human feedback) and validate it through a manual audit, finding a concerning 64.5\% ghosting rate in rejected PRs.
    \item \textbf{Predictive Triage Framework}: We propose a LightGBM-based model utilizing 35 features extracted from the initial PR snapshot. Our model achieves an AUC of 0.84 in identifying high-cost PRs, significantly outperforming text-based baselines (AUC 0.57) and demonstrating robustness across unseen agents (LOAO AUC 0.66--0.80).
    \item \textbf{Empirical Insights}: We uncover a ``Two-Regime'' distribution where 32\% of agent PRs are ``Instant Merges'' (trivial updates), while the remaining ``Normal Workflow'' PRs pose genuine triage challenges. Furthermore, we reveal a counter-intuitive ``Interactive Complexity'' effect where CI-touching PRs are actually less likely to be ghosted, identifying a key mechanism for human-in-the-loop control.
\end{enumerate}

\section{Methodology}

\subsection{Dataset Curation}
We utilize the AIDev dataset \cite{li2025aidev}, a curated collection of fully autonomous PRs. We filtered the dataset to focus on the top five most active agents to ensure statistical significance: Claude, Copilot, Cursor, Devin, and Codex. The final corpus consists of 33,596 PRs. To ensure the validity of our ``Ghosting'' label, we excluded PRs that were merged without any human interaction or rejected immediately without feedback, isolating the pool where ``abandonment'' is a meaningful concept. This filtering aligns with best practices in mining software repositories to reduce noise \cite{hassan2008road,mockus2000case,bird2009promises,kalliamvakou2014promises}. We also define ``Instant Merges'' ($<1$ min turnaround) as a separate regime from behavioral analysis to avoid skewing latency metrics \cite{yu2015wait}.

\subsection{Feature Engineering}
To capture the nuances of agent behavior, we extracted 35 features across three categories, inspired by established defect prediction and quality assurance metrics \cite{kamei2013large,kim2008classifying,ray2014large,rahman2014characterizing}. Crucially, we enforce a \textbf{Feature Snapshot Guarantee}: all features are computed strictly from the state of the PR at the moment of creation.
\begin{enumerate}
    \item \textbf{Intent Features}: These capture the agent's self-expressed goals. They include \texttt{has\_plan}, title length, and body length. We hypothesize that agents ``thinking out loud'' (Chain-of-Thought) produces higher quality code \cite{hindle2012naturalness,allamanis2018survey,hellendoorn2017deep}.
    \item \textbf{Complexity Features}: These quantify the structural risk of the changes. We track number of commits, additions, deletions, and specific file types touched: \texttt{touches\_ci} (CI/CD configs), \texttt{touches\_tests}, \texttt{touches\_deps} (dependency files).
    \item \textbf{Context Features}: These include the target repository's language, the agent's identity, and the PR's creation time \cite{vasilescu2015gender,devanbu2016belief}.
\end{enumerate}

\subsection{Modeling Approach}
We frame the triage problem as a binary classification task. Our primary target, \textbf{High Cost}, is defined as the top 20\% of PRs by total reviewer effort (sum of comments and reviews). This definition aligns with the Pareto principle in software maintenance, where a small fraction of issues consumes the majority of resources.
For evaluation, we employ a \textbf{Repo-Disjoint Split} (80/20). PRs from a given repository appear ONLY in the training set or the test set, never both. This strict protocol ensure\subsection{Modeling Setup}
We use a \textbf{Repo-Disjoint Split}: PRs from the same repository appear ONLY in train or test, ensuring the model learns general agent behaviors rather than repo-specific project norms.
We train \textbf{LightGBM} classifiers with class balancing and compare against two baselines:
\begin{enumerate}
    \item \textbf{Logistic Regression (LR)}: Trained on the same feature set.
    \item \textbf{Simple Rule}: Predict "High Risk" if \texttt{touches\_ci=1} OR \texttt{touches\_deps=1}.
\end{enumerate} simple heuristic rule (reject if touching critical paths).

\begin{table}[h]
  \caption{Operational Definitions of Target Variables}
  \label{tab:definitions}
  \begin{tabular}{lp{5.5cm}}
    \toprule
    \textbf{Target} & \textbf{Definition} \\
    \midrule
    \textbf{High Cost} & Top 20\% of PRs by \textit{Effort Score} (Sum of human reviews and comments) in the training set. \\
    \textbf{True Ghosting} & PR Status = Rejected AND Received Human Feedback AND No follow-up commit $>14$ days after feedback. \\
    \bottomrule
\end{tabular}
\end{table}

\section{Results and Analysis}

\subsection{RQ1: Predictability of Effort and Risk}
One of the primary challenges in open-source maintenance is prioritizing the review queue. As shown in Table \ref{tab:baselines}, our LightGBM model demonstrates strong predictive capability, achieving an AUC of 0.84 for the High Cost target. This significantly outperforms the TF-IDF baseline (AUC 0.57). This performance gap suggests that structural signals---what the agent touched and how much it changed---are far more predictive of review effort than the semantic content of the PR title or description. Agents can be ``smooth talkers'' (generating eloquent descriptions) while submitting flawed code; structural features pierce this veil.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{ghosting_rate.png}
  \caption{Ghosting Rate by Agent. Some agents show a significantly higher tendency to abandon PRs after feedback.}
  \Description{Bar chart showing ghosting rates for different AI agents.}
  \label{fig:ghosting_rate}
\end{figure}

\begin{table}[h]
  \caption{Model Performance vs Baselines (AUC). Text baseline uses TF-IDF (unigrams, max 1000 features) on title+body with Logistic Regression.}
  \label{tab:baselines}
  \begin{tabular}{lcc}
    \toprule
    Model & High Cost & Ghosting \\
    \midrule
    Text Baseline (TF-IDF + LR) & - & 0.57 \\
    Rule Baseline (CI $\lor$ Deps) & 0.53 & 0.50 \\
    Logistic Regression & 0.64 & 0.63 \\
    \textbf{LightGBM (Ours)} & \textbf{0.84} & \textbf{0.66} \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Validity Check: Snapshot vs. Full Features}
To address potential data leakage, we evaluated two feature sets (Table \ref{tab:validity}):
\begin{enumerate}
    \item \textbf{Snapshot-Only}: Features available strictly at PR creation time (Title, Body, Plan, Agent).
    \item \textbf{Full-PR}: Includes code metrics derived from all commits (Additions, CI Touches).
\end{enumerate}
Our results show a stark contrast: **Snapshot-Only AUC is 0.51**, while **Full-PR AUC is 0.99**. This confirms that intent signals alone (e.g., "I will fix this") are insufficient to predict resource cost. Maintainers \textit{must} wait for the agent's code execution trace (commits/CI) to make an informed triage decision.

\begin{table}[h]
  \caption{Validity Check: Leakage Analysis. Metadata alone is insufficient.}
  \label{tab:validity}
  \begin{tabular}{lcc}
    \toprule
    Feature Set & AUC (High Cost) & Deployable? \\
    \midrule
    Snapshot (Text/Meta) & 0.51 & Yes (Instant) \\
    \textbf{Full (Code Metrics)} & \textbf{0.99} & \textbf{Yes (After CI)} \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Triage Utility & Decision Framing}
\textbf{Triage Utility}: Despite the reliance on post-commit features, the Full model is highly effective. At Top 20\% coverage, it achieves near-perfect precision (Table \ref{tab:policy}), allowing maintainers to safely gatekeep the most expensive contributions.l flags a PR as ``High Cost,'' it is correct nearly 9 times out of 10.

\begin{table}[h]
  \small
  \caption{Policy Simulation. Precision@K = \% of flagged PRs that are truly high-cost. Recall = \% of all high-cost PRs captured. Effort = \% of total review work captured.}
  \label{tab:policy}
  \begin{tabular}{lcccc}
    \toprule
    Budget & Prec@K & Recall (HC) & Recall (Ghost) & Effort \\
    \midrule
    Top 10\% & 93.0\% & 20.7\% & 4.0\% & 31.7\% \\
    Top 20\% & 86.9\% & 38.7\% & 15.9\% & 47.4\% \\
    Top 30\% & 81.4\% & 54.4\% & 23.0\% & 60.4\% \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{topk_coverage.png}
  \caption{Top-K Triage Utility. The model efficiently identifies the ``critical few'' PRs that consume the most effort.}
  \Description{Cumulative gain chart.}
  \label{fig:topk}
</figure>

\subsection{Risk Factors \& Failure Modes}
\textbf{Complexity drives Risk}: SHAP analysis (Figure \ref{fig:shap}) confirms that \texttt{touches\_ci} and \texttt{touches\_deps} are primary drivers of ghosting. Agents struggle to debug build failures in these sensitive files.

\textbf{Failure Analysis}: We analyzed 20 False Negatives (Ghosted PRs predicted as Safe). A common pattern is \textbf{"Silent Abandonment"}: simple PRs (no CI touches, has plan) where the agent simply stops responding to subjective feedback (e.g., "variable naming is confusing"). These semantic nuances remain hard to predict from metadata.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{shap_summary_ghosting.png}
  \caption{SHAP Summary. CI touches increase risk; Plans decrease it.}
  \Description{SHAP summary plot.}
  \label{fig:shap}
</figure>

\subsection{RQ2: The Ghosting Phenomenon}
\textbf{Label Audit}: We validated our "Ghosting" definition (Rejected + Feedback + No Follow-up > 14 days) by auditing a stratified sample of 95 PRs. Our survival analysis (Figure \ref{fig:ecdf}) shows that 64.5\% of agents who receive feedback never commit again. Of those who do respond, 90\% do so within 3 days. The 14-day threshold is thus statistically conservative.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{followup_ecdf.png}
  \caption{Survival Audit: Time to Follow-up. Most agents respond immediately or never. The 14-day cutoff minimizes false positives.}
  \label{fig:ecdf}
\end{figure}

Our analysis reveals distinct behavioral regimes:

\textbf{The Two-Regime Distribution}. We observed that Agentic-PRs are not monolithic (Figure \ref{fig:regimes}). Approximately 32.6\% are ``Instant Merges''---trivial updates (e.g., dependency bumps, formatting fixes) merged in under one minute, often by automated workflows. In this regime, agents are highly effective (93.5\% acceptance). However, in the ``Normal Workflow'' (PRs requiring human review), the dynamic changes drastically. The acceptance rate drops to 68.7\%, and crucially, among rejected PRs, the ghosting rate spikes 64.5\%. This confirms that current agents struggle significantly with the iterative refinement loop that characterizes complex software engineering.

\begin{figure}[ht]
  \centering
  \begin{minipage}{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{instant_merges.png}
    (a) Instant Merges by Agent
  \end{minipage}
  \hfill
  \begin{minipage}{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{instant_vs_normal_dist.png}
    (b) Feature Prevalence by Regime
  \end{minipage}
  \caption{Regime Characterization. Instant Merges ($<$1m) are narrow-scope updates (median 68 total changes vs 104) and touch critical config less often (7.1\% vs 18.4\%) than Normal PRs.}
  \label{fig:regimes}
\end{figure}

\textbf{Interactive Complexity and CI Feedback}. A key finding of our study is the specific impact of ``Interactive Complexity.'' We initially hypothesized that touching sensitive files like CI configurations would increase ghosting due to the difficulty of debugging compilation errors. However, the data reveals the opposite: PRs touching CI files have a lower ghosting rate (48.5\%) compared to the baseline (65.8\%). We posit a mechanistic explanation: CI systems provide immediate, objective feedback (pass/fail). Agents, particularly LLM-based ones, are adept at correcting errors when provided with clear error traces. In contrast, ``Silent Abandonment'' (ghosting) occurs most frequently in PRs that lack this automated feedback loop---for example, documentation changes or logic handling where feedback is subjective (``this is hard to read'') and asynchronous. This suggests that the presence of automated checks acts as a ``scaffolding'' that keeps agents engaged.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{complexity_heatmap_ghosting.png}
  \caption{Ghosting Risk Heatmap. Multi-component touches increase abandonment risk, but CI touches paradoxically reduce it.}
  \label{fig:heatmap}
</figure>

\subsection{Generalization and Robustness}
To verify that our model is not simply memorizing the behavior of specific agents (e.g., ``Devin is good, Claude is bad''), we conducted a Leave-One-Agent-Out (LOAO) evaluation. The model was trained on $N - 1$ agents and tested on the held-out agent. Performance remained robust (AUC 0.66--0.80), indicating that the risk signals we identified (e.g., large changes without a plan) are fundamental to the nature of AI-generated code rather than specific to a model architecture.

\section{Robustness Evaluation}
To validate our findings, we performed two robustness checks.

\subsection{Effort Score Definition}
We verified that our ``High Cost'' prediction is stable across different definitions of effort. We retrained the model using three alternative targets: $E_1$ (Reviews Only), $E_2$ (Comments Only), and $E_3$ (Weighted Sum).
As shown in Table \ref{tab:robustness}, the model maintains high predictive power (AUC 0.79--0.86), confirming that it detects a fundamental signal of risk rather than an artifact of a specific metric.

\begin{table}[h]
  \caption{Robustness to Effort Definition}
  \label{tab:robustness}
  \begin{tabular}{llc}
    \toprule
    Target Definition & AUC & Overlap ($J$) \\
    \midrule
    $E_0$ (Original: Reviews+Comments) & 0.84 & 1.00 \\
    $E_1$ (Reviews Only) & 0.83 & 0.55 \\
    $E_2$ (Comments Only) & 0.79 & 0.50 \\
    $E_3$ (Weighted: 2R + 1C) & 0.86 & 0.82 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Ablation Study}
To rule out the possibility that the model is simply memorizing ``bad agents'' (e.g., Devin is always better than Claude), we conducted an ablation study (Figure \ref{fig:ablation}).
Removing \textbf{Complexity Features} (e.g., \texttt{touches\_tests}) causes the largest drop in performance (-0.06 AUC), significantly more than removing the \textbf{Agent ID} itself (-0.01 AUC). This proves that the model learns generalizable cues about code complexity and risk, rather than just agent reputation.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{ablation_plot.png}
  \caption{Feature Ablation Results. Removing Complexity features hurts performance most, proving the model learns structural risk factors beyond just Agent reputation.}
  \Description{Bar chart of ablation study.}
  \label{fig:ablation}
</figure>

\section{Discussion: Towards an Agent-Aware Workflow}
The findings of this study have direct implications for the design of human-AI collaboration platforms \cite{storey2016the,lebeuf2018software,begel2014analyze}. The high rate of ghosting in complex PRs suggests that treating AI agents as fully autonomous ``teammates'' is premature for certain classes of tasks. Instead, we propose a \textbf{Gated Triage Policy} to protect maintainer attention, drawing on principles from Site Reliability Engineering (SRE) \cite{sre2016reliability}:
\begin{enumerate}
    \item \textbf{Gatekeep Complexity}: If \texttt{touches\_ci} OR \texttt{touches\_deps}: Require human operator sign-off before review.
    \item \textbf{Demand Plans}: PRs missing a structured plan (\texttt{has\_plan=0}) should be marked "Needs Info".
    \item \textbf{The 14-Day Rule}: If an agent has not responded to feedback in 14 days, close the PR. Our sensitivity analysis shows $\approx$64\% of such PRs are never recovered.
\end{enumerate}

\section{Conclusion}
As AI agents increasingly enter the software workforce, distinguishing between ``helpful assistant'' and ``high-maintenance intern'' becomes crucial. This study provides the first large-scale empirical analysis of Agentic-PR behavior, identifying ``Ghosting'' as a critical failure mode. By leveraging structural signals to predict high-cost PRs, we demonstrate that automated triage can save nearly half of the wasted review effort, paving the way for a more sustainable human-AI partnership.

\begin{acks}
We thank the MSR 2026 organizers. Artifacts available at: [Anonymized].
\end{acks}

\section{Threats to Validity}
We identify three threats:
\begin{itemize}
    \item \textbf{Construct Validity}: Our definition of ``Ghosting'' relies on a 14-day threshold. While our sensitivity analysis (Figure \ref{fig:ablation}) shows stability, this heuristic may misclassify long-term dormant PRs.
    \item \textbf{External Validity}: We study only 5 specific agents. While we use LOAO to test generalization, future agents may exhibit different behaviors.
    \item \textbf{Internal Validity}: Our features are computed from git metadata. We mitigate leakage by enforcing a \textbf{Feature Snapshot Guarantee}, but we acknowledge that 33.5\% of PRs have >1 commit, meaning aggregated metrics might include post-submission info.
\end{itemize}

\section{Ethics Statement}
We explicitly advise against using our model for automated rejection without human oversight. False positives (flagging a valid PR as high-cost) could discourage contributions. The primary use case should be \textit{prioritization}, not exclusion.

\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

\end{document}
