\documentclass[sigconf,review,anonymous]{acmart}
\acmConference[MSR 2026]{MSR '26: Proceedings of the 23rd International Conference on Mining Software Repositories}{April 2026}{Rio de Janeiro, Brazil}

\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference info
\setcopyright{none}
\copyrightyear{2026}
\acmYear{2026}
\acmDOI{None}

\begin{document}

\title{Ghosting in the Machine: Predicting Wasted Review Effort in AI-Generated Pull Requests}

\author{Anonymous Author(s)}
\affiliation{
  \institution{MSR 2026 Mining Challenge}
  \country{Brazil}
}
\email{anonymous@example.com}

\renewcommand{\shortauthors}{Anonymous et al.}

\begin{abstract}
The emergence of autonomous coding agents has introduced a new dynamic in software engineering: ``AI Teammates'' that independently author Pull Requests (PRs). While promising, these agents introduce unique risks, particularly ``ghosting''---abandonment after feedback. Using 33,596 Agentic-PRs from the AIDev dataset, we identify two distinct regimes: ``Instant Merges'' (32\%) which are narrow-scope updates (median 68 lines), and ``Normal PRs'' where agents face genuine complexity. Our LightGBM models achieve an AUC of 0.84 for identifying high-cost PRs, outperforming a text baseline (AUC 0.57) and generalizing across unseen agents (LOAO AUC 0.66--0.80). We show that triage policies prioritizing the top 20\% of risky PRs can capture 47.4\% of total review effort on a repo-disjoint test set, enabling efficient human-in-the-loop workflows.
\end{abstract}

\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10011007.10011074.10011111.10011113</concept_id>
  <concept_desc>Software and its engineering~Software evolution</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Software evolution}

\keywords{AI Agents, Triage, Ghosting, Mining Software Repositories}

\maketitle

\section{Introduction}
As AI coding agents transition from assistants to active ``teammates'' \cite{li2025aidev,peng2023copilot}, a major friction point is the varying quality of ``Agentic-PRs'' \cite{tsay2014pr,gousios2014pr}. Unlike human contributors \cite{rigby2013convergent,thongtanunam2017review}, early autonomous agents may ``ghost'' maintainers---abandoning PRs after receiving complex feedback (Figure \ref{fig:ghosting_rate}).

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{ghosting_rate.png}
  \caption{Ghosting Rate by Agent. Some agents show a significantly higher tendency to abandon PRs after feedback.}
  \Description{Bar chart showing ghosting rates for different AI agents.}
  \label{fig:ghosting_rate}
\end{figure}

We address the MSR 2026 Mining Challenge by asking:
\begin{itemize}
    \item \textbf{RQ1}: Can we predict \textit{High Cost} and \textit{Ghosted} PRs at submission time?
    \item \textbf{RQ2}: What behavioral cues signal a high risk of abandonment?
\end{itemize}

We make three contributions:
\begin{enumerate}
    \item \textbf{Operationalization}: A rigorous ``True Ghosting'' definition with audit (64.5\% rate, stable across 7/14/30-day thresholds).
    \item \textbf{Predictive Triage}: A LightGBM model (AUC 0.84) with repo-disjoint evaluation and LOAO generalization (0.66--0.80).
    \item \textbf{Insights}: Two-regime discovery (Instant vs Normal), interactive complexity as risk driver, and failure mode analysis.
\end{enumerate}

\section{Methodology}

\subsection{Dataset \& Definitions}
We filter the AIDev dataset \cite{li2025aidev} for PRs authored by 5 agents (Claude, Copilot, Cursor, Devin, Codex), resulting in 33,596 PRs. We exclude ``Instant Merges'' ($<1$ min turnaround) from behavioral analysis to avoid skewing latency metrics (Figure \ref{fig:regimes}).

\textbf{Feature Snapshot Guarantee}: Intent features (\texttt{has\_plan}, title length, task type) are computed from the PR's initial submission text and cannot change. Aggregate features (\texttt{touches\_*}, additions) are computed from all commits; however, 66.5\% of PRs have only a single commit (pure snapshot), and 32.6\% are Instant Merges (no update opportunity). We acknowledge this limitation in Threats.



\begin{table}[h]
  \caption{Operational Definitions of Target Variables}
  \label{tab:definitions}
  \begin{tabular}{lp{5.5cm}}
    \toprule
    \textbf{Target} & \textbf{Definition} \\
    \midrule
    \textbf{High Cost} & Top 20\% of PRs by \textit{Effort Score} (Sum of human reviews and comments) in the training set. \\
    \textbf{True Ghosting} & PR Status = Rejected AND Received Human Feedback AND No follow-up commit $>14$ days after feedback. \\
    \bottomrule
\end{tabular}
\end{table}

\subsection{Modeling Setup}
We use a **Repo-Disjoint Split**: PRs from the same repository appear ONLY in train or test, ensuring the model learns general agent behaviors rather than repo-specific project norms.
We train **LightGBM** classifiers with class balancing and compare against two baselines:
\begin{enumerate}
    \item **Logistic Regression (LR)**: Trained on the same feature set.
    \item **Simple Rule**: Predict "High Risk" if \texttt{touches\_ci=1} OR \texttt{touches\_deps=1}.
\end{enumerate}

\section{Results}

\subsection{Model Performance}
LightGBM consistently outperforms baselines (Table \ref{tab:baselines}), confirming that agent triage requires non-linear combinations of features.

\begin{table}[h]
  \caption{Model Performance vs Baselines (AUC). Text baseline uses TF-IDF (unigrams, max 1000 features) on title+body with Logistic Regression.}
  \label{tab:baselines}
  \begin{tabular}{lcc}
    \toprule
    Model & High Cost & Ghosting \\
    \midrule
    Text Baseline (TF-IDF + LR) & - & 0.57 \\
    Rule Baseline (CI $\lor$ Deps) & 0.53 & 0.50 \\
    Logistic Regression & 0.64 & 0.63 \\
    \textbf{LightGBM (Ours)} & \textbf{0.84} & \textbf{0.66} \\
    \bottomrule
  \end{tabular}
\end{table}

\textbf{Triage Utility}: Table \ref{tab:policy} shows triage effectiveness on the repo-disjoint test set. At Top 20\%, the model achieves 86.9\% precision for identifying high-cost PRs while capturing 47.4\% of total effort---a favorable trade-off for capacity-constrained maintainers.

\textbf{Decision Framing}: Flagged PRs are routed to a lightweight gate: ``Require structured plan + CI pass before human review.'' This avoids spam by making the intervention cheap (no full code review needed for flagged cases that pass the gate).

\begin{table}[h]
  \small
  \caption{Policy Simulation. Precision@K = \% of flagged PRs that are truly high-cost. Recall = \% of all high-cost PRs captured. Effort = \% of total review work captured.}
  \label{tab:policy}
  \begin{tabular}{lcccc}
    \toprule
    Budget & Prec@K & Recall (HC) & Recall (Ghost) & Effort \\
    \midrule
    Top 10\% & 93.0\% & 20.7\% & 4.0\% & 31.7\% \\
    Top 20\% & 86.9\% & 38.7\% & 15.9\% & 47.4\% \\
    Top 30\% & 81.4\% & 54.4\% & 23.0\% & 60.4\% \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{topk_coverage.png}
  \caption{Top-K Triage Utility. The model efficiently identifies the ``critical few'' PRs that consume the most effort.}
  \Description{Cumulative gain chart.}
  \label{fig:topk}
\end{figure}

\subsection{Risk Factors \& Failure Modes}
\textbf{Complexity drives Risk}: SHAP analysis (Figure \ref{fig:shap}) confirms that \texttt{touches\_ci} and \texttt{touches\_deps} are primary drivers of ghosting. Agents struggle to debug build failures in these sensitive files.

\textbf{Failure Analysis}: We analyzed 20 False Negatives (Ghosted PRs predicted as Safe). A common pattern is **"Silent Abandonment"**: simple PRs (no CI touches, has plan) where the agent simply stops responding to subjective feedback (e.g., "variable naming is confusing"). These semantic nuances remain hard to predict from metadata.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{shap_summary_ghosting.png}
  \caption{SHAP Summary. CI touches increase risk; Plans decrease it.}
  \Description{SHAP summary plot.}
  \label{fig:shap}
\end{figure}

\subsection{Two-Regime Insight: The ``Instant Merge'' Phenomenon}
Our analysis reveals that Agentic-PRs fall into two distinct behavioral regimes based on turnaround time (Figure \ref{fig:regimes}).
\begin{enumerate}
    \item \textbf{Instant Merges} ($<1$ min, 32\% of data): Characterized by high acceptance (93.5\%) and narrow scope (often just dependency bumps or config tweaks).
    \item \textbf{Normal Workflow} ($>1$ min, 68\% of data): These involve human review latency. The acceptance rate drops to 68.7\%, and the ghosting rate among rejected PRs with human feedback is 64.1\% (substantially higher than the baseline of `no follow-up' in the instant-merge regime).
\end{enumerate}
This distinction is critical: treating all agent PRs as a single distribution masks the true difficulty of collaborative tasks.

\begin{figure}[ht]
  \centering
  \begin{minipage}{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{instant_merges.png}
    (a) Instant Merges by Agent
  \end{minipage}
  \hfill
  \begin{minipage}{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{instant_vs_normal_dist.png}
    (b) Feature Prevalence by Regime
  \end{minipage}
  \caption{Regime Characterization. Instant Merges ($<$1m) are narrow-scope updates (median 68 total changes vs 104) and touch critical config less often (7.1\% vs 18.4\%) than Normal PRs.}
  \label{fig:regimes}
\end{figure}

\subsection{Mechanism: Interactive Complexity}
Contrary to intuition, PRs touching CI/build files show \\textit{lower} ghosting rates (48.5% vs 65.8% for non-CI PRs; OR=0.49). We hypothesize CI-touching PRs trigger immediate pipeline feedback, forcing maintainers to respond quickly. Multi-component PRs have elevated ghosting, but this effect is dominated by the \\textit{absence} of CI signals rather than their presence.

\subsection{Generalization (LOAO)}
To test if our model learns generalizable structural cues rather than memorizing specific agents, we performed a Leave-One-Agent-Out evaluation. The model maintains strong performance on major agents (Copilot AUC 0.78, Codex AUC 0.80) even when they are excluded from training, and consistently outperforms the text baseline (0.57) across all unseen agents (min AUC 0.66). This confirms that risk signals like ``complexity without plan'' are universal.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{complexity_heatmap_ghosting.png}
  \caption{Ghosting Risk Heatmap. Multi-component touches increase abandonment risk.}
  \label{fig:heatmap}
\end{figure}

\section{Robustness Evaluation}
To validate our findings, we performed two robustness checks.

\subsection{Effort Score Definition}
We verified that our ``High Cost'' prediction is stable across different definitions of effort. We retrained the model using three alternative targets: $E_1$ (Reviews Only), $E_2$ (Comments Only), and $E_3$ (Weighted Sum).
As shown in Table \ref{tab:robustness}, the model maintains high predictive power (AUC 0.79--0.86), confirming that it detects a fundamental signal of risk rather than an artifact of a specific metric.

\begin{table}[h]
  \caption{Robustness to Effort Definition}
  \label{tab:robustness}
  \begin{tabular}{llc}
    \toprule
    Target Definition & AUC & Overlap ($J$) \\
    \midrule
    $E_0$ (Original: Reviews+Comments) & 0.84 & 1.00 \\
    $E_1$ (Reviews Only) & 0.83 & 0.55 \\
    $E_2$ (Comments Only) & 0.79 & 0.50 \\
    $E_3$ (Weighted: 2R + 1C) & 0.86 & 0.82 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Ablation Study}
To rule out the possibility that the model is simply memorizing ``bad agents'' (e.g., Devin is always better than Claude), we conducted an ablation study (Figure \ref{fig:ablation}).
Removing \textbf{Complexity Features} (e.g., \texttt{touches\_tests}) causes the largest drop in performance (-0.06 AUC), significantly more than removing the \textbf{Agent ID} itself (-0.01 AUC). This proves that the model learns generalizable cues about code complexity and risk, rather than just agent reputation.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{ablation_plot.png}
  \caption{Feature Ablation Results. Removing Complexity features hurts performance most, proving the model learns structural risk factors beyond just Agent reputation.}
  \Description{Bar chart of ablation study.}
  \label{fig:ablation}
\end{figure}

\section{Discussion: Actionable Policy}
Based on our findings, we propose a concrete Triage Policy for maintainers accepting Agentic-PRs:

\noindent\fbox{%
    \parbox{\linewidth}{%
        \textbf{Proposed Agentic Triage Policy}
        \begin{enumerate}
            \item \textbf{Gatekeep Complexity}: If \texttt{touches\_ci} OR \texttt{touches\_deps}: Require human operator sign-off before review.
            \item \textbf{Demand Plans}: PRs missing a structured plan (\texttt{has\_plan=0}) should be marked "Needs Info".
            \item \textbf{The 14-Day Rule}: If an agent has not responded to feedback in 14 days, close the PR. Our sensitivity analysis shows $\approx$64\% of such PRs are never recovered.
        \end{enumerate}
    }%
}

\begin{acks}
We thank the MSR 2026 organizers. Artifacts available at: [Anonymized].

\section{Threats to Validity \& Ethics}
\textbf{Internal Validity}: (1) Determining ``ghosting'' is difficult; agents might simply be slow. We mitigate this by using a relaxed 14-day threshold, which our sensitivity analysis confirms captures 64.5\% of cases that never recover (stable across 7/14/30 days). (2) Aggregate features (file touches) are computed from all PR commits, not initial submission. However, 66.5\% of PRs are single-commit, and intent features (has\_plan, title) are always snapshot.
\textbf{External Validity}: Our study is limited to 5 specific agents in the AIDev dataset. While LOAO results suggest learnable structural signals, future agents with different coding styles may require retraining.
\textbf{Construct Validity}: The text baseline uses TF-IDF (unigrams, max 1000 features, LR) on the same repo-disjoint split with balanced class weights.
\textbf{Ethics}: This study uses public, anonymized data from the AIDev dataset. No PII was processed. We analyze agent behavior, not human developer performance. Our policy is human-in-the-loop: agents are never auto-rejected.
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

\end{document}
