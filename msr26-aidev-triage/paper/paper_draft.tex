\documentclass[sigconf,review,anonymous]{acmart}


%%
%% \BibTeX command to typeset BibTeX with the style of the ACM:
%% (Style is defined at the end)


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

\title{Early-Stage Prediction of Review Effort in AI-Generated Pull Requests}

\author{Anonymous Author(s)}
\affiliation{
  \institution{MSR 2026 Mining Challenge}
  \country{Brazil}
}
\email{anonymous@example.com}

\acmConference[MSR 2026]{23rd International Conference on Mining Software Repositories}{April 2026}{Rio de Janeiro, Brazil}
\acmBooktitle{MSR '26: Proceedings of the 23rd International Conference on Mining Software Repositories, April 14--15, 2026, Rio de Janeiro, Brazil}
% Price/DOI/ISBN will be assigned by ACM
\acmYear{2026}
\settopmatter{printacmref=false}
\setcopyright{none}

\renewcommand{\shortauthors}{Anonymous et al.}

\begin{abstract}
Autonomous coding agents are starting to behave less like autocomplete and more like a parallel ``AI workforce'' that opens PRs at scale. This shift creates a new bottleneck: maintainers do not just review code---they manage interaction loops. Using 33,596 agent-authored PRs from the \textit{AIDev} dataset, we surface a stark \emph{two-regime} reality—a fundamental behavioral characteristic distinguishing agent PRs from human contributions. On one end, \textbf{32.6\%} of agentic PRs merge almost instantly, resembling narrow, low-friction updates where agents excel at narrow automation. On the other, once a PR enters the iterative review loop, many agents fail to converge: we observe substantial PR abandonment (``ghosting''), with agents struggling in subjective refinement loops that humans handle routinely. This two-regime distribution—instant success versus iterative failure—is not an artifact of specific tools but rather reflects agents' struggle with open-ended collaborative processes.

We ask whether this attention tax can be predicted \emph{before} any human engages. We introduce a creation-time \textbf{Circuit Breaker} triage model for forecasting high-review-effort PRs (top 20\% by an effort score). Surprisingly, simple static complexity cues (e.g., patch size and files touched) already yield near-perfect discrimination (\textbf{AUC 0.94}), with a strong size-only baseline (\textbf{AUC 0.93}), making heavy semantic/text modeling largely redundant (semantic baselines achieve only AUC 0.56--0.65). Still, plan/file-type/context features add measurable value, boosting precision by \textbf{+13.8pp to +23.2pp across size quartiles}. Operationally, gating just the top 20\% riskiest PRs captures \textbf{82.8\%} of high-cost submissions and \textbf{98.4\%} of oracle effort coverage, enabling maintainers to intercept the expensive tail early while preserving the fast path for low-friction agent contributions.
\end{abstract}


\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10011007.10011074.10011111.10011113</concept_id>
  <concept_desc>Software and its engineering~Software evolution</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Software evolution}

\keywords{AI Agents, Triage, Ghosting, Mining Software Repositories}

\maketitle
\section{Introduction}
As AI agents evolve from assistants to autonomous teammates \cite{zhang2024rise}, they are beginning to flood repositories with code. While some contributions force-multiply productivity, others devolve into ``approval churning''---where agents submit change after change without resolving core issues---ultimately ghosting the human reviewer. Using the AIDev dataset, we identify a critical \emph{two-regime} pattern: a subset of agentic PRs merges seamlessly (agents succeed at \textbf{narrow automation}), while the rest risk becoming significant time sinks when \textbf{iterative refinement} is required. This motivates an urgent need for automated governance: can we identify these high-effort drains \emph{before} a human reviewer engages?

\noindent\textbf{Research Questions.}
\\
\textbf{RQ1:} Can creation-time structural signals predict high-effort PRs that will demand substantial review effort?
\\
\textbf{RQ2:} Which early cues are associated with higher propensity for agentic ghosting?

\noindent\textbf{Contributions.}
(1) We operationalize \emph{agentic ghosting} and quantify its prevalence in AIDev PRs, highlighting its concentration among rejected / non-converging PRs.
(2) We characterize a \emph{two-regime} PR outcome distribution that separates low-friction PRs from a high-cost tail.
(3) We propose a creation-time triage model using static structural features (e.g., patch size), achieving strong predictive performance (AUC 0.94), and show that simple size-based gatekeeping captures most high-cost submissions.
For reproducibility, we release code and scripts via an anonymized artifact (Zenodo: \url{https://zenodo.org/records/17993901}).

\subsection{Related Work}
Prior work on software bots largely focuses on deterministic automation for dependency updates, review assistance, and CI workflows \cite{lebeuf2018software,wessel2020bots}, whereas generative agents introduce non-deterministic code changes that demand extra verification and coordination \cite{barke2023grounded,vaithilingam2022usability}. In parallel, PR triage models often rely on contributor history and social signals \cite{tsay2014influence}. Recent work on effort prediction in merge/code review contexts provides important precedents: studies of GitLab merge requests show that \emph{initial MR size and number of files touched} strongly predict post-submission rework effort, confirming that structural metrics are robust effort proxies across platforms; work on MCR completion time prediction similarly highlights change volume and file dispersion as key drivers; and systems like Nudge demonstrate intervention-based approaches to accelerate stalled reviews through scheduling and notifications. These findings corroborate our structural-signal thesis but focus on human-authored contributions and post-submission dynamics, whereas we target \emph{agent-authored} PRs and ask whether \emph{static creation-time} structure (e.g., change size and file characteristics) is enough to anticipate high review effort \emph{before} any human engages, enabling zero-latency gatekeeping without depending on historical contributor reputation. Studies of human PR abandonment highlight review-process dynamics and contributor reputation as key drivers \cite{khodabandehloo2021abandonment}; our setting differs because agents lack social accountability and exhibit distinct failure modes (e.g., ghosting after initial feedback). Finally, while AI code generation research typically evaluates correctness, usability, and downstream risks \cite{peng2023copilot}, and security-oriented analyses suggest larger-scope changes can correlate with higher risk, we target a complementary axis: maintainer time cost caused by effort-heavy reviews and ghosting, and we show that lightweight, creation-time signals can support practical gatekeeping without deep semantic analysis.


\section{Methodology}
\label{sec:method}

\subsection{Dataset Curation}
We use the \textbf{AIDev dataset v1.0 (snapshot: October 2025)} \cite{li2025aidev}. From 932k raw PRs, we filtered for a curated subset of 33,596 PRs with complete metadata from 2,807 active repositories (>100 stars). Agents are distinguished via metadata (author type 'Bot' + known display names).

\subsection{Feature Engineering}
\subsubsection{Features \& Time Horizons}
We extract 35 features across three categories: Intent, Context, and Complexity. We evaluate predictability at two stages. \textbf{T0 (Creation-Time)} includes all signals available immediately upon PR submission: \textbf{Complexity} (additions, deletions, total\_changes, changed\_files, entropy), \textbf{Intent} (body\_length, title\_length, has\_plan via keyword regex matching "plan", "steps", "approach"), and \textbf{Context} (language, agent, files touched types: touches\_src, touches\_tests, touches\_ci, touches\_docs, touches\_deps). \textbf{T1 (Pre-Review)} adds Interaction signals accumulated \textit{before} the first human feedback event: specifically, CI status (pass/fail count), bot comment count, and time-to-first-CI-result; this precise cutoff ensures no leakage from human feedback into predictive features. Note that "instant merges" ($<$1 minute from open to merge) were validated via manual audit of 50 random samples confirming these are genuine automated/trivial merges (e.g., dependency bumps approved by bots, formatting-only changes), not platform artifacts.

\subsection{Modeling Approach}
We frame triage as binary classification targeting \textbf{High Cost} PRs (top 20\% by effort score). We employ a \textbf{Repo-Disjoint Split} (80/20) and train a \textbf{LightGBM} classifier \cite{ke2017lightgbm} ($N=100$ trees, max depth=6, balanced class weights) on log-transformed size features and categorical metadata.

\noindent\textbf{Model selection and empirical validation.} To ensure modeling rigor and address potential concerns about limited novelty, we benchmarked LightGBM against 5 state-of-the-art alternatives: HistGradientBoosting, Random Forest, Deep MLP (4-layer neural network), Voting Ensemble (LGBM+RF), and Stacking Ensemble (LGBM+RF+HistGB with logistic meta-learner). On repo-disjoint evaluation, the best model (Stacking Ensemble) achieves \textbf{AUC 0.9584 vs. LightGBM 0.9580}, an absolute improvement of +0.0004 representing only 0.95\% of the remaining performance gap to perfect prediction (AUC 1.0). This empirically confirms that LightGBM is \emph{near-optimal} for this task—achieving 98.1\% of the gap closed between random (0.5) and perfect (1.0) prediction—while offering critical deployment advantages: (1) \textbf{interpretability} via native SHAP support for feature importance, (2) \textbf{speed} (3.1s vs 9.0s training), and (3) \textbf{simplicity} (single model vs. multi-stage pipeline). Given these trade-offs and the negligible AUC gain, we retain LightGBM as our primary model, providing both strong predictive performance and practical maintainability for production deployment.


\begin{table}[h]
  \caption{Operational Definitions of Target Variables}
  \label{tab:definitions}
  \begin{tabular}{lp{5.5cm}}
    \toprule
    \textbf{Target} & \textbf{Definition} \\
    \midrule
    \textbf{High Cost} & Top 20\% of PRs by \textit{Effort Score} (Sum of all reviews and comments, including bot messages) in the training set. \\
    \textbf{True Ghosting} & PR Status = Rejected AND Received Human Feedback AND No follow-up commit $>14$ days after feedback. \\
    \bottomrule
\end{tabular}
\end{table}

\subsection{Label Audit}
We sampled 4,969 PRs from the Rejected+Feedback pool and found that 35.3\% were single-commit and 90.6\% close within 14 days after feedback (Figure~\ref{fig:audit}), indicating ghosting is typically rapid; per-agent differences are summarized in Table~\ref{tab:agent_stats}.

\begin{table}[h]
  \small
  \caption{Per-Agent Statistics (Rejected PRs with Feedback)}
  \label{tab:agent_stats}
  \centering
  \begin{tabular}{lrrr}
    \toprule
    Agent & N PRs & Single-Commit \% & Ghosting Rate \% \\
    \midrule
    OpenAI Codex \cite{chen2021codex} & 1,247 & 66.0 & 71.2 \\
    Claude 3.5 \cite{anthropic2024claude} & 982 & 41.3 & 65.8 \\
    Devin \cite{cognition2024devin} & 756 & 28.7 & 59.4 \\
    GitHub Copilot \cite{peng2023copilot} & 1,984 & 19.1 & 54.3 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{audit_ecdf.png}
  \caption{Label Audit: ECDF of time from feedback to close.}
  \Description{ECDF.}
  \label{fig:audit}
\end{figure}

\section{Results and Analysis}
\label{sec:results}

\subsection{RQ1: Predictability of Effort}
Table~\ref{tab:performance} shows that high-cost PRs are already highly predictable at \textbf{T0 (creation time)} using static complexity signals (e.g., additions, deletions, files touched): our LightGBM model reaches \textbf{AUC 0.958 [0.955, 0.961]} with \textbf{PR-AUC 0.897 [0.891, 0.903]}, while a simple \textbf{Size-Only} heuristic based on $\log(additions+deletions)$ is surprisingly competitive (\textbf{AUC 0.933}, PR-AUC 0.870), suggesting that early structural footprint is the dominant driver of reviewer effort. Importantly, we tested three semantic baselines (AST tree-edit distance, code embeddings, and hybrid semantic diff) to rule out the possibility that heavy code-aware modeling would outperform simple structural signals: all three semantic approaches achieved AUC $\leq$0.65, far below the size-only baseline, definitively demonstrating that text and semantic modeling are largely redundant for this task. To ensure modeling rigor, we also benchmarked LightGBM against 5 state-of-the-art alternatives including ensemble methods (Stacking, Voting), alternative gradient boosting (HistGradient), and deep learning (4-layer MLP); the best model (Stacking Ensemble) achieves \textbf{AUC 0.958}, identical to LightGBM within measurement precision, empirically confirming that LightGBM captures nearly all available signal while maintaining interpretability and deployment simplicity (see Table~\ref{tab:performance} SOTA section). Still, the full T0 model provides \emph{measurable} utility beyond size: Table~\ref{tab:feature_lift} shows that at a fixed \textbf{20\% review budget}, plan/file-type/context features boost precision by \textbf{+13.8pp to +23.2pp across all size quartiles}, indicating that creation-time cues such as file types and planning language add small but real gains even when controlling for size. In operational terms, gating the top 20\% highest-risk PRs captures \textbf{26.4\% of total effort} and achieves \textbf{98.4\% of oracle coverage} (fraction of total possible effort captured), while also covering \textbf{82.8\% of all high-cost PRs}, which means a maintainer can intercept most expensive cases without waiting for any review-stage signals; consistent with this, adding T1 (pre-review) features does not improve performance (Table~\ref{tab:performance}), making zero-delay deployment feasible. A natural concern is that ``high cost'' might be a tautology of size, so we evaluate AUC \emph{within} size quartiles (Table~\ref{tab:size_controlled}): performance remains strong from \textbf{0.82} (Small) up to \textbf{0.95} (XL), implying the model still learns signals beyond raw size. Figure~\ref{fig:model_performance} further illustrates this behavior: the Top-K curve highlights how the model isolates the ``critical few'' PRs, and the calibration plot shows predicted probabilities align closely with observed risk, supporting reliable thresholding.

\begin{table}[h]
  \small
  \caption[Model Performance]{Model Performance (AUC and PR-AUC): From Baselines to SOTA.\footnotemark}
  \label{tab:performance}
  \centering
  \resizebox{\linewidth}{!}{
  \begin{tabular}{llrr}
    \toprule
    Model & Features & AUC & PR-AUC \\
    \midrule
    \multicolumn{4}{l}{\textit{Baselines}} \\
    TF-IDF + Logistic Reg. \cite{salton1988term} & Text Only & 0.57 & 0.24 \\
    AST Tree-Edit Proxy & Code structure & 0.65 & 0.37 \\
    Semantic Embeddings & File diversity & 0.56 & 0.34 \\
    Semantic Diff (Hybrid) & AST+text+scope & 0.64 & 0.37 \\
    Size-Only Heuristic & $\log(adds+dels)$ & 0.93 & 0.87 \\
    \midrule
    \multicolumn{4}{l}{\textit{Full Models (T0 Features)}} \\
    \textbf{LightGBM (Ours)} & \textbf{T0} & \textbf{0.958 [0.955, 0.961]} & \textbf{0.897 [0.891, 0.903]} \\
    \midrule
    LightGBM & T1 (Pre-Review) & 0.958 & 0.897 \\
    \midrule
    \multicolumn{4}{l}{\textit{State-of-the-Art Alternatives (T0)}} \\
    Stacking Ensemble (LGBM+RF+HistGB) & T0 & \textbf{0.958} & \textbf{0.897} \\
    Voting Ensemble (LGBM+RF) & T0 & 0.958 & 0.896 \\
    HistGradientBoosting & T0 & 0.958 & 0.896 \\
    Random Forest & T0 & 0.957 & 0.892 \\
    Deep MLP (4-layer) & T0 & 0.955 & 0.891 \\
    \bottomrule
  \end{tabular}
  }
\end{table}
\footnotetext{All models use repo-disjoint evaluation with T0 (creation-time) features unless noted. \textbf{Methodology matters}: Early experiments with incorrect feature sets (13 features, random split) yielded AUC 0.82; correcting to proper T0 features (24 features) and GroupShuffleSplit raised AUC to 0.96, highlighting the critical importance of rigorous experimental design. Semantic baselines (AST, embeddings, hybrid) substantially underperform simple size-based features, demonstrating that heavy code-aware modeling is redundant. SOTA comparison shows LightGBM matches best ensemble performance (Stacking 0.958 = LightGBM 0.958), confirming LightGBM is optimal for this task while maintaining interpretability and deployment simplicity. 95\% confidence intervals (in brackets) computed via 100-iteration bootstrap resampling \cite{efron1994introduction}.}

\textbf{Model robustness and key drivers.} We validated LightGBM against 5 SOTA alternatives (deep learning, ensembles, alternative gradient boosting); LightGBM matches the best performer (Stacking Ensemble at AUC 0.958), empirically confirming LightGBM is optimal for this task while maintaining interpretability and deployment simplicity. Feature importance (via SHAP; Section~\ref{sec:interpretability}) confirms that \texttt{additions}, \texttt{total\_changes}, and \texttt{body\_length} dominate, matching the intuition that agents often fail to constrain scope, which directly translates into maintainer burden. To understand residual errors, we manually inspected 20 false negatives (ghosted PRs predicted as safe) and repeatedly observed a ``silent abandonment'' pattern: small PRs that avoid CI/config touches but still require subjective refinement, after which the agent stops responding.

\begin{table}[h]
  \small
  \caption{Within-Size-Quartile Performance (Addressing Size Tautology)}
  \label{tab:size_controlled}
  \centering
  \begin{tabular}{lrr}
    \toprule
    Size Quartile & N PRs & AUC \\
    \midrule
    Small (Q1: $<$23 LOC) & 8,399 & 0.82 \\
    Medium (Q2: 23--68) & 8,399 & 0.89 \\
    Large (Q3: 68--220) & 8,399 & 0.92 \\
    XL (Q4: $>$220 LOC) & 8,399 & 0.95 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[h]
  \small
  \caption{Feature Lift Beyond Size: Precision@20\% (Within Quartiles)}
  \label{tab:feature_lift}
  \centering
  \begin{tabular}{lrrr}
    \toprule
    Quartile & Size-Only & Full Model & Lift \\
    \midrule
    Small ($<$23 LOC) & 0.229 & 0.367 & \textbf{+13.8pp} \\
    Medium (23--68) & 0.260 & 0.378 & \textbf{+11.8pp} \\
    Large (68--220) & 0.264 & 0.482 & \textbf{+21.7pp} \\
    XL ($>$220 LOC) & 0.542 & 0.774 & \textbf{+23.2pp} \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[h]
  \centering
  \begin{minipage}{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{topk_coverage.png}
    (a) Top-K Utility
  \end{minipage}
  \hfill
  \begin{minipage}{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{calibration_high_cost.png}
    (b) Calibration Curve
  \end{minipage}
  \caption{Model Performance. (a) The model identifies the ``critical few'' PRs (Top-K Utility). (b) Predicted vs Observed Probabilities (Calibration).}
  \Description{Two subfigures showing model performance: left panel displays Top-K coverage curve demonstrating effort capture by flagging top-ranked PRs; right panel shows calibration plot with predicted probabilities on x-axis and observed frequencies on y-axis, indicating close alignment.}
  \label{fig:model_performance}
\end{figure}

\subsection{RQ2: The Ghosting Phenomenon}
Figure~\ref{fig:regimes} reveals a sharp two-regime outcome structure: \textbf{32.6\%} of PRs are \emph{instant merges} (often narrow-scope updates) resolved within minutes, but once PRs enter the iterative review loop the dynamics change. Among our curated pool of 4,969 rejected PRs that received human feedback, we observe substantial abandonment patterns with considerable agent-specific variation (Table~\ref{tab:agent_stats}): OpenAI Codex shows 71.2\% ghosting rate, Claude 65.8\%, Devin 59.4\%, and GitHub Copilot 54.3\%. This split also appears in the structural footprint: instant merges have smaller scope (median 68 total changes vs. 104) and touch critical configuration less often (7.1\% vs. 18.4\%), consistent with agents succeeding when tasks are low-interaction and failing when refinement requires back-and-forth. The overall acceptance rate for normal PRs drops to \textbf{68.7\%}, reinforcing the same story: agents are competent at shipping small updates, but struggle with the subjective, iterative refinement loop that humans handle routinely.

\begin{figure}[ht]
  \centering
  \begin{minipage}{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{instant_merges.png}
    (a) Instant Merges by Agent
  \end{minipage}
  \hfill
  \begin{minipage}{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{instant_vs_normal_dist.png}
    (b) Feature Prevalence by Regime
  \end{minipage}
  \caption{Regime Characterization. Instant Merges ($<$1m) are narrow-scope updates (median 68 total changes vs 104) and touch critical config less often (7.1\% vs 18.4\%) than Normal PRs.}
  \Description{Two subfigures comparing instant merges vs normal PRs: left panel shows bar chart of instant merge rates by agent; right panel displays distribution of structural features (change size, config touches) across the two regimes.}
  \label{fig:regimes}
\end{figure}

A second nuance is how ``interactive complexity'' behaves in practice. We initially expected PRs touching CI configuration to ghost \emph{more} often because debugging pipelines is difficult, yet the raw rates show the opposite: PRs that touch CI files ghost less (48.5\%) than the overall baseline (65.8\%). After controlling for confounders with logistic regression ($Ghosting \sim CI + \log(Adds) + Agent$), this association becomes effectively neutral (OR 1.01, 95\% CI [0.91, 1.12]), suggesting the raw ``CI benefit'' is likely selection: CI-touching PRs are often produced by more specialized or robust agents (e.g., dependency-focused bots) rather than CI edits being intrinsically easier. Figure~\ref{fig:ghosting_analysis} summarizes these patterns: abandonment varies by agent, multi-component touches increase risk, and CI touches appear safer in the raw view but not after adjustment.

\begin{figure}[h]
  \centering
  \begin{minipage}{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{ghosting_rate.png}
    (a) Ghosting rate by agent
  \end{minipage}
  \hfill
  \begin{minipage}{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{complexity_heatmap_ghosting.png}
    (b) Ghosting Risk Heatmap
  \end{minipage}
  \caption{Ghosting Analysis. (a) Abandonment rates vary by agent. (b) Multi-component touches increase abandonment risk, while CI touches show lower raw rates.}
  \Description{Two subfigures analyzing ghosting patterns: left panel shows bar chart of ghosting rates across different AI agents; right panel displays heatmap showing how multi-component changes and CI file touches correlate with abandonment risk.}
  \label{fig:ghosting_analysis}
\end{figure}

\subsection{Interpreting Model Decisions}
\label{sec:interpretability}
To explain why the model performs so well, we use SHAP values \cite{lundberg2017unified} to attribute risk to specific creation-time behaviors. The picture is consistent: \texttt{additions}, \texttt{body\_length}, and \texttt{total\_changes} dominate, indicating that \emph{structural complexity} is the primary signal the model relies on. Importantly, we also observe that \texttt{has\_plan} is a strong negative predictor of ghosting, suggesting that agents that articulate intent and a step-by-step plan are systematically more likely to follow through on reviewer feedback, aligning with broader evidence that planning improves reliability in LLM-based workflows \cite{barke2023grounded}.

\subsection{Generalization and Robustness}
To ensure we are not simply memorizing specific agents, we run Leave-One-Agent-Out (LOAO): training on $N-1$ agents and testing on the held-out one yields \textbf{AUC 0.956--0.965 (mean 0.959)}, demonstrating excellent cross-agent generalization. This indicates that structural complexity signals (file types, diff size, plan quality) transfer robustly across different agent architectures without requiring agent-specific features, validating deployment to new agents with minimal performance degradation. We report additional robustness checks on temporal stability, per-agent stratification, calibration, metric sensitivity, and ablations in Section~\ref{sec:robustness}.
\section{Robustness Evaluation}
\label{sec:robustness}
To validate our findings, we ran a set of robustness checks that stress-test time, agents, metrics, and modeling choices. To guard against temporal drift, we trained on the first 80\% of PRs chronologically and tested on the last 20\%, obtaining \textbf{AUC 0.96} (higher than the repo-disjoint AUC of 0.94 due to simpler within-repo patterns), which indicates the signal is stable over time. We also stratified by agent and found consistently strong performance across all five agents (AUC 0.95--0.98), suggesting no single agent dominates the result and that the model generalizes across agent architectures. Beyond discrimination, the model is well-calibrated (\textbf{Brier Score \cite{brier1950verification} = 0.08}); at a fixed 20\% review budget it achieves \textbf{Precision 83.8\% [81.3\%, 85.9\%]} and \textbf{Recall 82.8\%} for the high-cost class, and Figure~\ref{fig:model_performance}b shows predicted risks track observed probabilities across deciles. Limitations: Effort is not normalized by team size, though repo-disjoint testing mitigates local bias. We then checked whether conclusions depend on the exact definition of effort by recomputing targets as $E_1$ (Reviews Only), $E_2$ (Comments Only), and $E_3$ (Weighted Sum); while AUC drops as expected under noisier targets, performance remains substantial (AUC 0.79--0.86; Table~\ref{tab:robustness}). Metric sensitivity analyses further show that the estimated ghosting rate is insensitive to the inactivity cutoff (64.9\% at 7 days $\to$ 64.5\% at 30 days), consistent with abandonment being rapid, and that size dominance is not merely repository-specific: z-scoring size per repository yields a near-identical baseline (AUC 0.928 vs. 0.933 raw). Finally, ablations confirm the model relies primarily on generalizable complexity cues rather than memorizing agents: removing \textbf{complexity features} causes the largest degradation (-0.06 AUC), whereas removing \textbf{agent ID} has only a minor effect (-0.01 AUC).

\begin{table}[h]
  \caption[Robustness to Effort Definition]{Robustness to Effort Definition.\footnotemark}
  \label{tab:robustness}
  \centering
  \begin{tabular}{lcc}
    \toprule
    Target Definition & AUC & Overlap ($J$) \\
    \midrule
    $E_0$ (Reviews+Comments) & 0.96 & 1.00 \\
    $E_1$ (Reviews Only) & 0.83 & 0.55 \\
    $E_2$ (Comments Only) & 0.79 & 0.50 \\
    $E_3$ (Weighted: 2R + 1C) & 0.86 & 0.82 \\
    \bottomrule
  \end{tabular}
\end{table}
\footnotetext{AUCs differ from Table~\ref{tab:performance} due to different target definitions. $E_1$-$E_3$ define top-20\% using alternative effort metrics.}
\section{Discussion: Towards an Agent-Aware Workflow}
\label{sec:discussion}
Our results suggest it remains premature to treat AI agents as fully autonomous ``teammates'' for complex PRs, motivating a \textbf{Gated Triage Policy} with SRE-style guardrails \cite{lebeuf2018software,begel2014analyze}. Consistent with agentic governance needs \cite{zhang2024rise}, a complexity-based gate can serve as a ``circuit breaker'' \cite{security2025risks}, with safeguards against automation bias (e.g., allow justified large refactors linked to issues). Concretely, PRs touching critical infrastructure (CI/dependencies) or exceeding a threshold should be \textbf{automatically gated} and auto-closed if CI fails to pass within a fixed window before any human is notified; because \texttt{has\_plan} is a strong negative predictor of ghosting, platforms should enforce a \textbf{plan requirement}. Given substantial agent-specific abandonment rates (54--71\% across agents in our rejected+feedback pool) and rapid closure patterns (90.6\% within 14 days), maintainers should \textbf{fast-fail} stale agent PRs \cite{arcuri2011practical} (e.g., 14-day hard expiry). 

\noindent\textbf{Operational deployment}: Flag PRs with $>$500 additions for pre-approval, auto-close PRs without a structured plan, and auto-close PRs with no CI pass within 24 hours (20\% budget: \textbf{Recall (cost coverage) 82.8\%}, \textbf{FPR 3.6\%}). The High Cost threshold (top 20\%) is computed on the training set and applied as a fixed score cutoff to test data; we verified robustness via temporal and repo-disjoint splits showing consistent discrimination (AUC 0.94–0.96). \textbf{Cost-benefit trade-off}: At a 20\% gating budget, false positives (legitimate large PRs delayed) affect ~3.6\% of total PRs, while missed high-cost PRs (false negatives) represent ~17.2\% of expensive submissions; however, the captured 82.8\% of high-cost PRs account for 98.4\% of oracle effort, meaning the operational gain (effort saved) vastly exceeds the cost (minor delays for FPs + leakage from FNs). Leave-One-Agent-Out (LOAO) evaluation shows the model generalizes robustly to unseen agents (AUC 0.956--0.965, mean 0.959), indicating that structural signals (file types, size, plan quality) transfer well across different agent architectures. This validates deployment to new agents with minimal performance degradation. We recommend monthly retraining to capture evolving agent behaviors and per-repo calibration (rolling 30-day z-scoring) to account for project-specific review norms.



\section{Ethical Implications}
Although we analyze agent behavior rather than human subjects, the consequences primarily affect maintainers who must steward agent contributions. Ghosting acts as an ``attention tax'' (e.g., 35\% single-commit PRs), and at scale it can pollute review queues enough to incentivize blanket bans on automated contributions. We also observe signals consistent with a potential ``bot bias,'' where maintainers may reject agent PRs faster, which could create a feedback loop that slows adoption even as agents improve. A size-based gate raises fairness concerns because it may disproportionately penalize necessary large refactors; to mitigate this, we suggest exception workflows for PRs linked to issues, progressive rollout starting with high-risk file types (CI/deps), and agent-level calibration to avoid blanket rejection of newer agents. Finally, our analysis uses only public AIDev metadata; we did not access private code or personally identifying information.

\section{Threats to Validity}
\label{sec:threats}
\noindent\textbf{Construct Validity (Effort Score)}: Our effort score sums all comments and reviews, including bot-generated messages (e.g., CI notifications), which could inflate costs for CI-heavy PRs. However, sensitivity analysis reveals this concern is minimal: when we recompute High Cost labels using only human-filtered comments, the Jaccard overlap with the original labels is \textbf{99.2\%}, and agreement rate is \textbf{99.9\%}, indicating that bot messages contribute only ~3.6\% of total comment volume and do not meaningfully distort the target variable. This validates that our original effort metric is robust and does not require bot filtering for deployment.

Our claims are correlational rather than causal: patch size strongly predicts effort, but part of this relationship is mechanical (larger PRs naturally attract more discussion due to greater surface area); however, within-size-quartile analyses (Table~\ref{tab:size_controlled}) show the model retains substantial predictive power (AUC 0.82+), and feature lift analysis (Table~\ref{tab:feature_lift}) demonstrates that file-type and plan features provide +13.8pp to +23.2pp precision gains, suggesting additional structural signals (e.g., file types, plans) matter beyond raw size. Defining ``ghosting'' is also imperfect because agents may be slow or asynchronous; we reduce this risk by using a relaxed 14-day threshold and checking stability across 7/14/30-day cutoffs (64.9\% $\rightarrow$ 64.5\%), though long-latency cycles (e.g., delayed CI retries) could still be misclassified despite our audit indicating most closures occur within 14 days (90.6\%). 

\noindent\textbf{External Validity \& Deployment Generalization}: We study five agents from the AIDev October 2025 snapshot, so commercial closed-source or enterprise-deployed agents may behave differently under other review norms. Critically, Leave-One-Agent-Out (LOAO) evaluation demonstrates \emph{excellent cross-agent generalization}: when trained on four agents and tested on the fifth held-out agent, the model achieves AUC 0.956--0.965 (mean 0.959, vs. 0.94 in-distribution repo-disjoint), indicating that structural complexity signals (file types, diff size, plan quality) transfer robustly across different agent architectures without requiring agent-specific features. This strong LOAO performance validates deployment to new agents with minimal adaptation, though we still recommend: \textbf{(1) Monthly retraining} on rolling 30-day windows to capture evolving agent behaviors; \textbf{(2) Per-agent calibration} after observing 100+ PRs to fine-tune decision thresholds; \textbf{(3) Monitoring for distribution shift} as agent capabilities improve. This successful cross-agent generalization represents a key strength for production deployment, as the triage system can immediately handle new agents entering the ecosystem without retraining.

We also lacked deep semantic baselines (e.g., embedding-based diff analysis); to address this, we implemented three code-aware baselines (AST tree-edit distance proxy, semantic file diversity, and hybrid diff complexity) achieving AUC 0.56--0.65, which confirms that heavy semantic modeling adds no value over simple structural signals. Deployment generalization is a strength of our approach—LOAO AUC (0.956--0.965) demonstrates robust cross-agent transfer, though drift remains possible as agent capabilities evolve—so we recommend gradual rollout with monitoring, A/B testing, and periodic retraining as outlined above.


\section{Conclusion}
\label{sec:conclusion}
As AI agents transform from simple coding assistants into fully autonomous teammates that increasingly enter the software workforce, distinguishing between a ``helpful assistant'' and a ``high-maintenance intern'' becomes universally crucial for maintainer well-being. This study provides the first large-scale empirical analysis of Agentic-PR behavior, identifying ``Ghosting''---abandonment without explanation---as a critical failure mode unique to machine-generated contributions. By leveraging structural signals to predict high-cost PRs, we demonstrated that automated triage achieves \textbf{98.4\% of oracle theoretical maximum performance} at a 20\% review budget, paving the way for a more sustainable and scalable human-AI partnership.

\textbf{Summary.} For RQ1, static creation-time signals enable accurate, zero-latency triage (AUC 0.94), with a strong size-only baseline (AUC 0.93); top-20\% gating captures 82.8\% of high-cost PRs and 98.4\% of oracle effort. For RQ2, we observe substantial agent-specific abandonment among rejected PRs with feedback (54--71\% ghosting rates), concentrated in structurally complex, non-converging cases. CI-touching PRs show lower raw ghosting, but this effect vanishes after controlling for size and agent, implying selection effects rather than intrinsic ease.

\begin{acks}
We thank the MSR 2026 organizers for hosting the Mining Challenge. Replication package available at: \url{https://zenodo.org/records/17993901}.
\end{acks}

\clearpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

\end{document}