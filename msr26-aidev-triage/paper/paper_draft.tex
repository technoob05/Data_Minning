\documentclass[sigconf,review,anonymous]{acmart}

%%
%% \BibTeX command to typeset BibTeX with the style of the ACM:
\bibliographystyle{ACM-Reference-Format}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

\title{Early-Stage Prediction of Review Effort in AI-Generated Pull Requests}

\author{Anonymous Author(s)}
\affiliation{
  \institution{MSR 2026 Mining Challenge}
  \country{Brazil}
}
\email{anonymous@example.com}

\acmConference[MSR 2026]{23rd International Conference on Mining Software Repositories}{April 2026}{Rio de Janeiro, Brazil}
\acmBooktitle{MSR '26: Proceedings of the 23rd International Conference on Mining Software Repositories, April 14--15, 2026, Rio de Janeiro, Brazil}
\acmPrice{15.00}
\acmDOI{10.1145/3540250.3549112}
\acmISBN{978-1-4503-9421-5/26/04}

\renewcommand{\shortauthors}{Anonymous et al.}

\begin{abstract}
  Automatic code generation agents are flooding open-source repositories with high-volume contributions. While these agents promise efficiency, they risk overwhelming maintainers with low-quality or ``ghosted'' Pull Requests (PRs).    In this paper, we analyze 33,596 PRs from the \textit{AIDev} dataset (v1.0) to characterize agent behavior. We identify a ``Two-Regime'' distribution: agents either merge instantly or enter a high-cost, high-ghosting cycle (64.5\% ghosting rate in rejected PRs). We propose a \textbf{Gated Triage} policy. Our experiments show that simple metadata signals are insufficient (AUC 0.54), but incorporating dynamic signals (e.g., CI feedback, file complexity) boosts prediction of high-effort PRs to \textbf{AUC 0.96}. A policy gating the top 20\% risky PRs captures 47.4\% of total maintainer effort and identifies 22.9\% of wasted effort on unmerged contributions.
\end{abstract}

\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10011007.10011074.10011111.10011113</concept_id>
  <concept_desc>Software and its engineering~Software evolution</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Software evolution}

\keywords{AI Agents, Triage, Ghosting, Mining Software Repositories}

\maketitle

\section{Introduction}
Autonomous AI coding agents (Claude, Devin, GitHub Copilot) are becoming active collaborators, independently submitting Pull Requests \cite{li2025aidev,peng2023copilot,chen2021codex,brown2020language,touvron2023llama,li2022competition,fried2023incoder,austin2021program}. However, unlike human contributors who typically follow social norms \cite{rigby2013convergent,thongtanunam2017review,dabbish2012social,steinmacher2015barriers}, agents exhibit a critical failure mode we term "ghosting": submitting PRs but failing to respond to feedback, potentially overwhelming maintainers. With 932K+ agent-authored PRs in the AIDev dataset \cite{li2025aidev}, mitigating this "Denial-of-Service" on attention is urgent.

\subsection{Related Work}
\textbf{Software Bots.} Prior work examined deterministic bots for dependency updates \cite{lebeuf2018software}, code review \cite{wessel2018power,wessel2021effects}, and CI tasks \cite{wessel2019how}. Wessel et al. \cite{wessel2020bots} found bots improve throughput but require maintainer oversight. Unlike these deterministic tools, generative AI agents produce non-deterministic outputs requiring verification \cite{barke2023grounded,vaithilingam2022usability}.

\textbf{Code Review \& Triage.} Traditional PR triage leverages social signals (reputation, history) \cite{tsay2014influence,yu2015wait}. However, AI agents lack social accountability, necessitating triage based on structural signals (complexity, file types) as we demonstrate.

\textbf{AI Code Generation.} Recent studies explore developer-AI interaction \cite{bird2009promises,bacchelli2013expectations} and productivity \cite{peng2023copilot}. However, these focus on initial acceptance. We study the novel failure mode of "ghosting" (abandonment after feedback).

\textbf{Gap \& Contribution.} We provide the first empirical characterization of ghosting (64.5\% in rejected PRs) and demonstrate the necessity of interactive signals for triage, as early metadata alone is weak (AUC 0.54).


This work directly addresses the MSR 2026 Mining Challenge question on predicting Agentic-PR outcomes. We investigate:
\begin{itemize}
    \item \textbf{RQ1 (Predictability)}: Can early-stage structural signals (e.g., patch characteristics) predict high-effort triage inputs?
    \item \textbf{RQ2 (Risk Factors)}: What behavioral cues signal a higher propensity for ghosting?
\end{itemize}

\textbf{Contributions:} (1) We operationalize "Agentic Ghosting" and find a 64.5\% abandonment rate in rejected PRs. (2) We demonstrate that effective triage requires interactive signals (AUC 0.96), as creation-time metadata is insufficient (AUC 0.54). (3) We uncover a "Two-Regime" distribution where simple PRs are effective, but complex ones require strict gatekeeping.

For reproducibility, we release our code and data at Zenodo\footnote{Replication package: \url{https://anonymous.4open.science/r/msr26-aidev-triage}} (Anonymous).

\section{Methodology}
\label{sec:method}

\subsection{Dataset Curation}
We use the \textbf{AIDev dataset v1.0 (snapshot: October 2025)} \cite{li2025aidev}. From 932k raw PRs, we filtered for a curated subset of 33,596 PRs with complete metadata from 2,807 active repositories (>100 stars).

\subsection{Feature Engineering}
\subsubsection{Feature Availability Timing}
Our features are extracted from the PR's initial state after the agent's first commit(s) but \textbf{BEFORE} human review begins. This "early-stage" window allows automated triage to run before maintainers invest effort. While not strictly "submission-time" (feature vector includes patch characteristics), these signals enable proactive resource allocation.

\begin{table}[t]
\small
\caption{Feature Categories (35 Total)}
\label{tab:features}
\begin{tabular}{lp{5cm}}
\toprule
Category & Key Features \\
\midrule
Intent (3) & has\_plan, title\_length, body\_length \\
Complexity (28) & additions, deletions, commits, \newline
                  touches\_ci, touches\_tests, touches\_deps \\
Context (4) & agent\_id, language, time, stars \\
\bottomrule
\end{tabular}
\end{table}

We extract 35 features across three categories (Table~\ref{tab:features}): (1) \textbf{Intent}, (2) \textbf{Complexity}, and (3) \textbf{Context}. All features satisfy a \textbf{Feature Snapshot Guarantee}, computed strictly from creation-time metadata.

\subsection{Modeling Approach}
We frame the triage problem as a binary classification task. Our primary target, \textbf{High Cost}, is defined as the top 20\% of PRs by total reviewer effort (sum of comments and reviews). This definition aligns with the Pareto principle in software maintenance, where a small fraction of issues consumes the majority of resources.
For evaluation, we employ a \textbf{Repo-Disjoint Split} (80/20). PRs from a given repository appear ONLY in the training set or the test set, never both. This strict protocol ensures that our model learns generalizable signals of agent behavior rather than memorizing project-specific norms or confidentially overfitting to a specific repo's style. We compare our LightGBM model with class balancing against two baselines: a Logistic Regression model trained on TF-IDF vectors of the PR content (representing a traditional NLP approach) and a simple heuristic rule (reject if touching critical paths).

\begin{table}[t!]
  \caption{Operational Definitions of Target Variables}
  \label{tab:definitions}
  \begin{tabular}{lp{5.5cm}}
    \toprule
    \textbf{Target} & \textbf{Definition} \\
    \midrule
    \textbf{High Cost} & Top 20\% of PRs by \textit{Effort Score} (Sum of human reviews and comments) in the training set. \\
    \textbf{True Ghosting} & PR Status = Rejected AND Received Human Feedback AND No follow-up commit $>14$ days after feedback. \\
    \bottomrule
\end{tabular}
\end{table}

\subsection{Label Audit}
To validate our ``True Ghosting'' definition, we sampled 4,969 PRs from the Rejected+Feedback pool. We found that 35.3\% were single-commit (no follow-up activity), and 90.6\% were closed within 14 days of receiving feedback (Figure~\ref{fig:audit}). This confirms that ``ghosting'' is a fast phenomenon: agents that do not respond within a few days rarely recover. We also observed agent-specific patterns: OpenAI Codex had the highest single-commit rate (66%), while Copilot showed only 19%.

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.6\linewidth]{audit_ecdf.png}
  \caption{Label Audit: ECDF of time from feedback to close.}
  \Description{ECDF.}
  \label{fig:audit}
\end{figure}

\section{Results and Analysis}
\label{sec:results}

\subsection{RQ1: Predictability of Effort and Risk}
One of the primary challenges in open-source maintenance is prioritizing the review queue. As shown in Table \ref{tab:performance}, our LightGBM model demonstrates strong predictive capability only when using full interactive features (AUC 0.96). The Snapshot model (AUC 0.54) fails to outperform the TF-IDF baseline (AUC 0.57). This negative result highlights that "Intent" (metadata) is cheap; only "Action" (code/complexity) signals risk.

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.7\linewidth]{ghosting_rate.png}
  \caption{Ghosting Rate by Agent. Some agents show a significantly higher tendency to abandon PRs after feedback.}
  \Description{Bar chart showing ghosting rates for different AI agents.}
  \label{fig:ghosting_rate}
\end{figure}

\begin{table}[t!]
  \centering
  \small
  \caption{Model Performance (AUC) \& Leakage Control. Snapshot relies on metadata available at creation (preventing leakage). Full-PR incorporates dynamic lifecycle signals.}
  \label{tab:performance}
  \begin{tabular}{llr}
    \toprule
    Model & Setting & AUC (95\% CI) \\
    \midrule
    Baseline (TF-IDF+LR) & Snapshot & 0.57 [0.56, 0.58] \\
    \textbf{LightGBM (Ours)} & \textbf{Snapshot} & 0.53 [0.52, 0.55] \\
    \textbf{LightGBM (Ours)} & \textbf{Full-PR} & \textbf{0.96} [0.95, 0.96] \\
    \bottomrule
  \end{tabular}
\end{table}

\textbf{Triage Utility Analysis}: To translate these metrics into actionable value, we simulated a triage policy. If a maintainer has a limited ``budget'' to review only the top 20\% most risky PRs, our model captures 47.4\% of the total effort. Crucially, the model identifies \textbf{wasted effort}: 22.9\% of all review comments are spent on PRs that are never merged. Our policy flags these high-risk candidates effectively.

\textbf{Leakage Control \& Validity}: To ensure realistic evaluation, we rigidly separate `Snapshot' features (available $t=0$) from `Full-PR' features. The low performance of Snapshot models (AUC 0.54) serves as a negative result: metadata alone is insufficient to predict agent risk. However, the high performance of Full-PR (AUC 0.96) validates the \textbf{Gated Triage} hypothesis: risk manifests through \textit{interaction} (CI failures, large diffs), not initial intent.

\textbf{Feature Extraction settings}: We report two settings: (1) \textbf{Snapshot} (Deployable): Uses only PR title, body, and plan availability at creation. (2) \textbf{Full-PR} (Robustness): Includes dynamic metrics (file touches, additions) accumulated during the PR lifecycle. As shown in Table \ref{tab:performance}, early signals (Snapshot) provide weak discriminative power (AUC 0.54), confirming that \textit{interactive complexity}---how the PR evolves---is the primary driver of cost.



\begin{figure}[t!]
  \centering
  \includegraphics[width=0.7\linewidth]{topk_coverage.png}
  \caption{Top-K Triage Utility. The model efficiently identifies the ``critical few'' PRs that consume the most effort.}
  \Description{Cumulative gain chart.}
  \label{fig:topk}
\end{figure}

\subsection{Risk Factors \& Failure Modes}
\textbf{Complexity drives Risk}: SHAP analysis (omitted for space) confirms that \texttt{touches\_ci} and \texttt{touches\_deps} are primary drivers of ghosting.
\textbf{Failure Analysis}: We analyzed 20 False Negatives (Ghosted PRs predicted as Safe). A common pattern is \textbf{Silent Abandonment}: simple PRs (no CI touches) where the agent ignores subjective feedback.

\subsection{RQ2: The Ghosting Phenomenon}
\textbf{Two-Regime Distribution and Survival.} Our audit reveals a sharp dichotomy. Figure \ref{fig:regimes} shows that 32.6\% of PRs are ``Instant Merges'' (often dependency bumps) accepted in minutes. However, among the rejected PRs, the ghosting rate spikes to 64.5\%. To understand this, we performed a survival analysis on the Time-to-Resolution (omitted for space).



The acceptance rate drops to 68.7\% for normal PRs, confirming that agents struggle with the iterative refinement loop.

\begin{figure}[ht]
  \centering
  \begin{minipage}{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{instant_merges.png}
    (a) Instant Merges by Agent
  \end{minipage}
  \hfill
  \begin{minipage}{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{instant_vs_normal_dist.png}
    (b) Feature Prevalence by Regime
  \end{minipage}
  \caption{Regime Characterization. Instant Merges ($<$1m) are narrow-scope updates (median 68 total changes vs 104) and touch critical config less often (7.1\% vs 18.4\%) than Normal PRs.}
  \label{fig:regimes}
\end{figure}

\textbf{Interactive Complexity and CI Feedback}. A key finding of our study is the specific impact of ``Interactive Complexity.'' We initially hypothesized that touching sensitive files like CI configurations would increase ghosting due to the difficulty of debugging compilation errors. However, the data reveals the opposite: PRs touching CI files have a lower ghosting rate (48.5\%) compared to the baseline (65.8\%). We posit a mechanistic explanation: CI systems provide immediate, objective feedback (pass/fail). Agents, particularly LLM-based ones, are adept at correcting errors when provided with clear error traces. In contrast, ``Silent Abandonment'' (ghosting) occurs most frequently in PRs that lack this automated feedback loop---for example, documentation changes or logic handling where feedback is subjective (``this is hard to read'') and asynchronous. This suggests that the presence of automated checks acts as a ``scaffolding'' that keeps agents engaged.

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.6\linewidth]{complexity_heatmap_ghosting.png}
  \caption{Ghosting Risk Heatmap. Multi-component touches increase abandonment risk.}
  \label{fig:heatmap}
\end{figure}

\textbf{Confounder Analysis.} While raw rates suggested CI touches might reduce ghosting, a logistic regression controlling for PR size and agent ID revealed no significant effect (Odds Ratio 1.01, $p=0.83$). The perceived safety of CI-touching PRs is likely confounded by their tendency to be smaller or atomic adjustments. Thus, we cannot claim CI feedback \textit{causes} engagement; rather, complex changes inherently carry higher ghosting risk regardless of file type.

\subsection{Generalization and Robustness}
To verify that our model is not simply memorizing the behavior of specific agents, we conducted a Leave-One-Agent-Out (LOAO) evaluation. The model was trained on $N - 1$ agents and tested on the held-out agent. Performance remained robust (AUC 0.70--0.92), indicating that the risk signals we identified (e.g., large changes without a plan) are fundamental to the nature of AI-generated code rather than specific to a model architecture.

\section{Robustness Evaluation}
\label{sec:robustness}
To validate our findings, we performed two robustness checks.

\subsection{Effort Score Definition}
We verified that our ``High Cost'' prediction is stable across different definitions of effort. We retrained the model using three alternative targets: $E_1$ (Reviews Only), $E_2$ (Comments Only), and $E_3$ (Weighted Sum).
As shown in Table \ref{tab:robustness}, the model maintains high predictive power (AUC 0.79--0.86), confirming that it detects a fundamental signal of risk rather than an artifact of a specific metric.

\begin{table}[t!]
  \caption{Robustness to Effort Definition}
  \label{tab:robustness}
  \begin{tabular}{llc}
    \toprule
    Target Definition & AUC & Overlap ($J$) \\
    \midrule
    $E_0$ (Original: Reviews+Comments) & 0.84 & 1.00 \\
    $E_1$ (Reviews Only) & 0.83 & 0.55 \\
    $E_2$ (Comments Only) & 0.79 & 0.50 \\
    $E_3$ (Weighted: 2R + 1C) & 0.86 & 0.82 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Ablation Study}
To rule out "memorization" of specific agents, we conducted an ablation study. Removing \textbf{Complexity Features} (e.g., \texttt{touches\_tests}) causes the largest drop in performance (-0.06 AUC), significantly more than removing the \textbf{Agent ID} itself (-0.01 AUC). This proves that the model learns generalizable cues about code complexity and risk.

\section{Discussion: Towards an Agent-Aware Workflow}
\label{sec:discussion}
The findings of this study have direct implications for the design of human-AI collaboration platforms \cite{storey2016the,lebeuf2018software,begel2014analyze}. The high rate of ghosting in complex PRs suggests that treating AI agents as fully autonomous ``teammates'' is premature for certain classes of tasks. Instead, we propose a \textbf{Gated Triage Policy} to protect maintainer attention, drawing on principles from Site Reliability Engineering (SRE) \cite{sre2016reliability}:
\begin{enumerate}
    \item \textbf{Automated Gatekeeping}: PRs that touch critical infrastructure (CI, Dependencies) or exceed a certain complexity threshold should be gated behind an automated check. If the agent cannot pass the CI within a set timeframe, the PR should be auto-closed before a human is ever notified.
    \item \textbf{The ``Plan'' Requirement}: Our feature importance analysis shows that the presence of a structured plan (\texttt{has\_plan}) is a strong negative predictor of ghosting. Platforms should enforce a template requiring agents to output a step-by-step plan before generating code.
    \item \textbf{Fast-Fail Thresholds}: Given the 64.5\% ghosting rate and the ``stable'' nature of abandonment (most agents who don't reply in 7 days never reply), maintainers should be empowered to close stale Agentic-PRs aggressively \cite{arcuri2011practical,jorgensen2007systematic}. We recommend a 14-day hard expiry for agent PRs lacking activity.
\end{enumerate}

\section{Ethical Implications}
Our work analyzes the behavior of AI agents, not humans, but the implications extend to the maintainers who must steward them.
\textbf{Maintainer Burnout.} The "Ghosting" phenomenon (35\% single-commit PRs) represents a tax on attention. As agents scale, this pollution could force maintainers to ban automated contributions entirely.
\textbf{Bias.} Findings suggest a "bot bias" where maintainers reject agent PRs faster. This feedback loop could hinder agent adoption even as they improve.
\textbf{Privacy.} We analyzed only public metadata from the AIDev dataset; no PII or private code was accessed.

\section{Threats to Validity}
\label{sec:threats}
\textbf{Internal Validity}: Determining ``ghosting'' is difficult; agents might simply be slow. We mitigate this by using a relaxed 14-day threshold.
\textbf{External Validity}: Our study is limited to 5 specific agents. Commercial closed-source agents may behave differently.
\textbf{Construct Validity}: The text baseline uses TF-IDF on the same repo-disjoint split.

\section{Conclusion}
\label{sec:conclusion}
As AI agents increasingly enter the software workforce, distinguishing between ``helpful assistant'' and ``high-maintenance intern'' becomes crucial. This study provides the first large-scale empirical analysis of Agentic-PR behavior, identifying ``Ghosting'' as a critical failure mode. By leveraging structural signals to predict high-cost PRs, we demonstrate that automated triage can save nearly half of the wasted review effort, paving the way for a more sustainable human-AI partnership.

\textbf{Ans.\ to RQ1}: Early-stage structural signals are noisy (AUC 0.54), but interactive signals (Full-PR) reliably predict high-cost Agentic-PRs (AUC 0.96). This enables triage policies that capture 47.4\% of wasted effort by reviewing only the top 20\% of flagged PRs.

\textbf{Ans.\ to RQ2}: PRs touching CI/build files showed lower raw ghosting rates, but this effect disappears after controlling for PR size (OR 1.01, $p>0.05$). This suggests that complexity, rather than specific file types, is the primary driver of agent abandonment.

\begin{acks}
We thank the MSR 2026 organizers for hosting the Mining Challenge. Replication package available at: [Anonymized for review].
\end{acks}

\clearpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

\end{document}
